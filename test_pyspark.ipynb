{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdUE5jveYbtD"
      },
      "source": [
        "# Examen PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kFs_gTGYbtN"
      },
      "source": [
        "Instrucciones: Lea cuidadosamente las preguntas, escriba el código correspondiente y ejecútelo para mostrar sus resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNL4FiPmYbtP"
      },
      "source": [
        "#### Importante: Todos los ejercicios deberán realizarse con funciones de NumPy, Pandas o PySpark (no podrán crearse vistas temporales para realizarse en SQL, salvo que se indique lo contrario)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp2IiyAcYbtQ"
      },
      "source": [
        "## Bloque 1: Spark Core"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJQ8ELxSYbtR"
      },
      "source": [
        "1.1 Utilizando NumPy, construya un arreglo con 50 elementos aleatorios distribuidos de forma normal con media 50 y desviación estándar 10. Imprima el arreglo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iW3d7TP_YbtS"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uq1XYnqcYbtV",
        "outputId": "13a544b0-4b70-400f-b41d-570a6c1b5860"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51.41211895 41.30382136 49.40267228 43.74319984 44.9896942  48.86832578\n",
            " 35.35454696 48.39321791 61.81990937 57.16836919 51.26872697 48.16641319\n",
            " 41.04109791 35.2360609  61.51464773 68.49301323 49.94368585 50.27991755\n",
            " 79.61585714 50.11499132 52.26859163 38.16070342 22.67376466 68.72406406\n",
            " 27.64495173 19.75590009 43.71880852 36.57553935 64.08030082 39.46789899\n",
            " 53.28395941 26.58124005 63.12519008 57.1151237  43.0041912  42.57461162\n",
            " 60.11752267 62.15897516 53.85281622 49.03915124 74.47144248 46.5119436\n",
            " 43.30367002 71.66917706 49.88421323 55.88142623 35.71516196 33.00220337\n",
            " 42.27604202 48.06667009]\n"
          ]
        }
      ],
      "source": [
        "media = 50\n",
        "desv_estandar = 10\n",
        "n = 50\n",
        "\n",
        "arreglo = np.random.normal(loc=media, scale=desv_estandar, size=n)\n",
        "print(arreglo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD24qDc7Ybta"
      },
      "source": [
        "1.2. Construya el objeto de Spark (Core) que le permita trabajar con objetos RDD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhqRSV9_Ybtb",
        "outputId": "ac854b86-36a8-4d2b-de15-d60ae456f86e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=da989016e11554faaff370a03137829cc5f5cbc9cc50ce4a16e0dd50b45cde7d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "arq-x9IQYbtc"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WwRQ8A7_Ybte"
      },
      "outputs": [],
      "source": [
        "sc = SparkContext(\"local\", \"testSpark\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sc.stop()"
      ],
      "metadata": {
        "id": "tRkn8SHPZ1uj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ybAbh0oYbtg"
      },
      "source": [
        "1.3. Convierta el arreglo de NumPy a un RDD con 2 particiones. Muestre los\n",
        "primeros 5 elementos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir el arreglo de NumPy en un RDD con 2 particiones\n",
        "rdd_2 = sc.parallelize(arreglo, 2)\n",
        "print(rdd_2.take(5))\n"
      ],
      "metadata": {
        "id": "7CFYMNpeaLvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648ad0fa-12be-49a1-c0d7-617e55ce7473"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[51.41211895005393, 41.30382135624631, 49.40267228330854, 43.74319983860883, 44.98969420089373]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS2OMFXKYbth"
      },
      "source": [
        "1.4. Suponiendo que los datos de la lista miden grados Fahrenheit, aplique una función lambda al RDD que convierta las mediciones a grados Centígrados. Muestre los primeros 5 elementos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lpsUHb1Ybtk"
      },
      "source": [
        "C = (F - 32) * 5 / 9"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f_to_c = lambda f: (f - 32) * 5/9\n",
        "\n",
        "rdd_celsius = rdd_2.map(f_to_c)\n",
        "print(rdd_celsius.take(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mDMary1bkc8",
        "outputId": "82790bab-239c-473a-e65f-124e24461526"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10.784510527807738, 5.168789642359061, 9.668151268504744, 6.523999910338239, 7.216496778274294]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0BOc6HFYbtl"
      },
      "source": [
        "1.5. Utilice una función Lambda para mostrar únicamente las temperaturas mayores a 15 grados Centigrados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lambda para filtrar las temperaturas mayores a 15 grados Celsius\n",
        "t_mayores_15C = rdd_celsius.filter(lambda temp: temp > 15)\n",
        "print(t_mayores_15C.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajFMQVo9ctr2",
        "outputId": "0f5feb27-5cc9-443d-aeb1-ae5ed57dd5ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[16.56661631657016, 16.39702651869861, 20.27389623848664, 26.453253967414, 20.402257812256305, 17.822389343971327, 17.29177226854383, 15.620845928891644, 16.75498619925976, 23.59524582369334, 22.038431701485734]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRWa60aIYbts"
      },
      "source": [
        "1.6. Calcule la temperatura media en grados Centígrados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calcular la temperatura media ¡\n",
        "t_media_celsius = rdd_celsius.mean()\n",
        "print(t_media_celsius)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJWrV2DZdHvN",
        "outputId": "e9562ee8-38a4-44a4-cddf-778ca8bccb32"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.36483935913651\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVNWyYjQYbtt"
      },
      "source": [
        "1.7. Obtenga las 3 temperaturas más altas en grados Centígrados."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t_mas_altas = rdd_celsius.top(3)\n",
        "print(t_mas_altas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TB925LHPdeCg",
        "outputId": "9579d249-e576-4edf-c892-8e57aa04cd45"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[26.453253967414, 23.59524582369334, 22.038431701485734]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMcx8oW_Ybtu"
      },
      "source": [
        "## Bloque 2: Spark SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYs1otlhYbtv"
      },
      "source": [
        "2.1. Utilizando Numpy, construya un arreglo con 50 números enteros entre 1 y 3 (1 y 3 incluidos)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arreglo_2 = np.random.randint(1, 4, 50)\n",
        "print(arreglo_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6y-rjt7eF-d",
        "outputId": "d26a1a4f-6f95-4a95-b119-e813d5f78fb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3 1 2 1 2 1 2 3 1 2 3 2 1 2 3 2 3 3 3 2 1 3 3 3 2 2 3 2 2 2 3 2 1 2 3 2 2\n",
            " 2 3 3 3 1 3 1 2 2 3 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCJ77hKKYbtv"
      },
      "source": [
        "2.2. Construya un dataframe en Pandas utilizando los arreglos de 2.1 y 1.1. Asigne los nombres \"dia\" y \"temp\". Muestre los primeros 5 elementos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "djG_jp2ne9Sb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"dia\": arreglo_2, \"temp\": arreglo})\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihlpv0_TexgD",
        "outputId": "7d4163b7-4973-4c99-b7f1-5e000c854638"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   dia       temp\n",
            "0    3  51.412119\n",
            "1    1  41.303821\n",
            "2    2  49.402672\n",
            "3    1  43.743200\n",
            "4    2  44.989694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghzSYXWyYbtw"
      },
      "source": [
        "2.3. Construya el objeto de Spark (SQL) que le permita trabajar con los dataframes de Spark."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"testEx2\").getOrCreate()"
      ],
      "metadata": {
        "id": "c_rIlmAsfmIi"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDUkkVerYbtx"
      },
      "source": [
        "2.4. Convierta el dataframe de Pandas a un dataframe de Spark, definiendo explícitamente el esquema/estructura (utilice el tipo entero para el día y el tipo doble para la temperatura). Muestre los primeros 5 registros."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convertir pandas df en un Spark df\n",
        "df_spark = spark.createDataFrame(df)\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxV0UBkSgDWM",
        "outputId": "ff567ba5-6b09-47cb-9406-421617bdae49"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------+\n",
            "|dia|             temp|\n",
            "+---+-----------------+\n",
            "|  3|51.41211895005393|\n",
            "|  1|41.30382135624631|\n",
            "|  2|49.40267228330854|\n",
            "|  1|43.74319983860883|\n",
            "|  2|44.98969420089373|\n",
            "+---+-----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRfuzZo3Ybtz"
      },
      "source": [
        "2.5. Partiendo del dataframe en Spark, construya un dataframe con el promedio de temperatura agrupado por día. El dataframe deberá contener únicamente las columnas \"dia\" y \"temp_prom\" (con esos nombres). Muestre la tabla resultante."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_temp_pr_bc = df_spark.groupBy(\"dia\").agg({\"temp\": \"avg\"})\n",
        "df_temp_prom = df_temp_pr_bc.withColumnRenamed(\"avg(temp)\", \"temp_prom\")\n",
        "df_temp_prom.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CLkjQ8NhFeD",
        "outputId": "0937aa98-5bd4-4885-a396-36f886669124"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "|dia|         temp_prom|\n",
            "+---+------------------+\n",
            "|  1| 49.47468100835718|\n",
            "|  3| 51.02088362811127|\n",
            "|  2|46.538173245799854|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DftedOK5Ybt1"
      },
      "source": [
        "2.6. Repita el ejercicio anterior registrando una vista temporal y ejecutando el código SQL correspondiente. Muestre la tabla resultante."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "0W_YK6vtjTGJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# registrar el df como una vista temporal\n",
        "df_spark.createOrReplaceTempView(\"tabla_t\")\n",
        "\n",
        "#\n",
        "sql = \"\"\"\n",
        "SELECT dia, AVG(temp) AS temp_pers\n",
        "FROM tabla_t\n",
        "GROUP BY dia\n",
        "\"\"\"\n",
        "\n",
        "consulta = spark.sql(sql)\n",
        "consulta.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwJGaoVlh86_",
        "outputId": "db730ac1-4036-4164-a842-11b46208ec41"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+\n",
            "|dia|         temp_pers|\n",
            "+---+------------------+\n",
            "|  1| 49.47468100835718|\n",
            "|  3| 51.02088362811127|\n",
            "|  2|46.538173245799854|\n",
            "+---+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGpD24BfYbt2"
      },
      "source": [
        "2.7. Combine los valores del dataframe anterior con el original. El dataframe resultante no deberá contener columnas repetidas y tendrá que estar ordenado de forma ascendente por día y temperatura. Muestre los primeros 5 elementos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n"
      ],
      "metadata": {
        "id": "bj_q7HLelhcu"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 df: df_temp_prom\n",
        "# 2 df: consulta\n",
        "\n",
        "joined_df = df_temp_prom.join(consulta, on=\"dia\", how=\"inner\").orderBy(\"dia\")\n",
        "joined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVhxEKNWl_Bd",
        "outputId": "2eb05020-64b0-4153-9515-7081fb647cef"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------------+\n",
            "|dia|         temp_prom|         temp_pers|\n",
            "+---+------------------+------------------+\n",
            "|  1| 49.47468100835718| 49.47468100835718|\n",
            "|  2|46.538173245799854|46.538173245799854|\n",
            "|  3| 51.02088362811127| 51.02088362811127|\n",
            "+---+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSFCp4woYbt3"
      },
      "source": [
        "2.8. Añada una columna adicicional con la diferencia entre la temperatura y su media. Asigne el nombre \"resid\". Muestre los primeros 5 elementos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "nlM7uukBnvSz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_resid = joined_df.withColumn(\"resid\", F.col(\"temp_prom\") - F.col(\"temp_pers\"))\n",
        "df_resid.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFJKVKemnbi9",
        "outputId": "1c4ac4d1-b230-422c-bc25-dad806eb7cc8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------+------------------+-----+\n",
            "|dia|         temp_prom|         temp_pers|resid|\n",
            "+---+------------------+------------------+-----+\n",
            "|  1| 49.47468100835718| 49.47468100835718|  0.0|\n",
            "|  2|46.538173245799854|46.538173245799854|  0.0|\n",
            "|  3| 51.02088362811127| 51.02088362811127|  0.0|\n",
            "+---+------------------+------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNV3d9j5Ybt4"
      },
      "source": [
        "### 2.9. Guarde el dataframe resultante en formato JSON. En caso de que el archivo ya exista, deberá sobreescribirse."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_resid.write.mode(\"overwrite\").json(\"./temp_prom.json\")"
      ],
      "metadata": {
        "id": "Jm6pF_Tdoeks"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H3GfJUYG_Kev"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}